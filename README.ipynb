{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060fbe29",
   "metadata": {},
   "source": [
    "# Weather Data Pipeline Project\n",
    "\n",
    "This project implements an end-to-end data pipeline to collect, process, and visualize historical weather data from the Open Meteo Weather API.\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "The pipeline consists of the following components:\n",
    "\n",
    "1. **Data Collection**: Lambda function fetches data from Open Meteo API and sends it to Kinesis Firehose\n",
    "2. **Data Storage**: Firehose delivers data to Amazon S3 in JSON format\n",
    "3. **Data Cataloging**: AWS Glue Crawler catalogs the data for querying\n",
    "4. **Data Transformation**: Glue jobs transform the data into Parquet format with calculated fields\n",
    "5. **Data Quality Checks**: Validation of transformed data to ensure quality\n",
    "6. **Production Data**: Final transformation creates a production-ready dataset\n",
    "7. **Data Visualization**: Grafana dashboard visualizes temperature trends\n",
    "\n",
    "### Architecture Diagram\n",
    "\n",
    "![Architecture Diagram](pics/architecture_diagram.png)\n",
    "\n",
    "### Flowchart\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    %% Data Source\n",
    "    API[Open Meteo API]\n",
    "    \n",
    "    %% Data Collection\n",
    "    LAMBDA[Lambda Function]\n",
    "    FIREHOSE[Kinesis Firehose]\n",
    "    \n",
    "    %% Data Storage\n",
    "    S3_RAW[S3 Raw Data]\n",
    "    S3_TRANSFORM[S3 Transformed Data]\n",
    "    S3_PROD[S3 Production Data]\n",
    "    \n",
    "    %% Data Cataloging & Processing\n",
    "    CRAWLER[Glue Crawler]\n",
    "    CATALOG[Glue Data Catalog]\n",
    "    DEL_JOB[Delete Job]\n",
    "    CREATE_JOB[Create Job]\n",
    "    DQ_JOB[DQ Check Job]\n",
    "    PUBLISH_JOB[Publish Job]\n",
    "    \n",
    "    %% Data Analysis & Visualization\n",
    "    ATHENA[Amazon Athena]\n",
    "    GRAFANA[Grafana Dashboard]\n",
    "    \n",
    "    %% Connections with 90-degree angles\n",
    "    API --> LAMBDA\n",
    "    LAMBDA --> FIREHOSE\n",
    "    FIREHOSE --> S3_RAW\n",
    "    \n",
    "    S3_RAW --> CRAWLER\n",
    "    CRAWLER --> CATALOG\n",
    "    \n",
    "    CATALOG --> DEL_JOB\n",
    "    DEL_JOB --> CREATE_JOB\n",
    "    \n",
    "    CREATE_JOB --> |transform| S3_TRANSFORM\n",
    "    S3_TRANSFORM --> DQ_JOB\n",
    "    \n",
    "    DQ_JOB --> PUBLISH_JOB\n",
    "    PUBLISH_JOB --> |transform| S3_PROD\n",
    "    \n",
    "    S3_PROD --> ATHENA\n",
    "    ATHENA --> GRAFANA\n",
    "    \n",
    "    %% Custom styling\n",
    "    classDef dataSource fill:#3498db,stroke:#2980b9,stroke-width:2px,color:white\n",
    "    classDef processing fill:#e74c3c,stroke:#c0392b,stroke-width:2px,color:white\n",
    "    classDef storage fill:#2ecc71,stroke:#27ae60,stroke-width:2px,color:white\n",
    "    classDef analytics fill:#9b59b6,stroke:#8e44ad,stroke-width:2px,color:white\n",
    "    classDef visualization fill:#f39c12,stroke:#d35400,stroke-width:2px,color:white\n",
    "    \n",
    "    %% Apply classes\n",
    "    class API dataSource\n",
    "    class LAMBDA,FIREHOSE,CRAWLER,DEL_JOB,CREATE_JOB,DQ_JOB,PUBLISH_JOB processing\n",
    "    class S3_RAW,S3_TRANSFORM,S3_PROD storage\n",
    "    class CATALOG,ATHENA analytics\n",
    "    class GRAFANA visualization\n",
    "    \n",
    "    %% Add labels\n",
    "    subgraph \"Data Source\"\n",
    "        API\n",
    "    end\n",
    "    \n",
    "    subgraph \"Data Collection\"\n",
    "        LAMBDA\n",
    "        FIREHOSE\n",
    "    end\n",
    "    \n",
    "    subgraph \"Data Storage\"\n",
    "        S3_RAW\n",
    "        S3_TRANSFORM\n",
    "        S3_PROD\n",
    "    end\n",
    "    \n",
    "    subgraph \"Data Cataloging\"\n",
    "        CRAWLER\n",
    "        CATALOG\n",
    "    end\n",
    "    \n",
    "    subgraph \"Data Processing\"\n",
    "        DEL_JOB\n",
    "        CREATE_JOB\n",
    "        DQ_JOB\n",
    "        PUBLISH_JOB\n",
    "    end\n",
    "    \n",
    "    subgraph \"Data Analysis\"\n",
    "        ATHENA\n",
    "    end\n",
    "    \n",
    "    subgraph \"Data Visualization\"\n",
    "        GRAFANA\n",
    "    end\n",
    "```\n",
    "\n",
    "## Components\n",
    "\n",
    "### 1. Lambda Function\n",
    "\n",
    "The Lambda function collects historical weather data from Open Meteo API and forwards it to Kinesis Firehose.\n",
    "\n",
    "```python\n",
    "# Sample code showing Lambda function structure\n",
    "import json\n",
    "import boto3\n",
    "import urllib3\n",
    "import datetime\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    http = urllib3.PoolManager()\n",
    "    r = http.request(\"GET\", \"https://api.open-meteo.com/v1/forecast?latitude=40.7143&longitude=-74.006&daily=temperature_2m_max&temperature_unit=fahrenheit&timezone=America%2FNew_York&start_date=2025-01-01&end_date=2025-04-16\")\n",
    "    \n",
    "    # Process data and send to Firehose\n",
    "    # ...\n",
    "```\n",
    "\n",
    "![Lambda Function Configuration Screenshot](pics/lambda-screenshot.png)\n",
    "\n",
    "### 2. Kinesis Firehose\n",
    "\n",
    "Firehose delivers the data to S3 in JSON format. The delivery stream is configured to buffer and batch records for efficiency.\n",
    "\n",
    "![Firehose Configuration Screenshot](pics/firehose-config.png)\n",
    "\n",
    "### 3. S3 Storage\n",
    "\n",
    "The raw data is stored in S3 buckets with the following structure:\n",
    "\n",
    "- **Source Data**: `open-meteo-weather-data-parquet-bucket-04142025`\n",
    "- **Production Data**: `parquet-weather-table-prod-04142025`\n",
    "- **Query Results**: `query-results-location-de-proj-04152025`\n",
    "\n",
    "![S3 Bucket Contents Screenshot](pics/s3-contents.png)\n",
    "\n",
    "### 4. AWS Glue Crawler\n",
    "\n",
    "The Glue Crawler catalogs the data in the AWS Glue Data Catalog for querying.\n",
    "\n",
    "Configuration:\n",
    "- Database: `weather-database-04142025`\n",
    "- Table prefix: `weather_`\n",
    "- Source location: `s3://open-meteo-weather-data-parquet-bucket-04142025/`\n",
    "\n",
    "![Glue Crawler Configuration Screenshot](pics/crawler-config.png)\n",
    "\n",
    "### 5. AWS Glue Jobs\n",
    "\n",
    "The workflow orchestrates several Glue jobs for data transformation:\n",
    "\n",
    "#### a. Delete Job\n",
    "Cleans up previous data and tables.\n",
    "\n",
    "```python\n",
    "# delete_parquet_weather_table_s3_athena.py\n",
    "import sys\n",
    "import boto3\n",
    "\n",
    "# Drop existing tables\n",
    "client = boto3.client('athena')\n",
    "queryString = \"DROP TABLE IF EXISTS table_name\"\n",
    "# ...\n",
    "```\n",
    "\n",
    "#### b. Create Job\n",
    "Transforms the data and adds calculated fields.\n",
    "\n",
    "```python\n",
    "# create_parquet_weather_table_glue_job.py\n",
    "import sys\n",
    "import boto3\n",
    "\n",
    "# Create new table with transformations\n",
    "client = boto3.client('athena')\n",
    "create_query = \"\"\"\n",
    "CREATE TABLE database_name.table_name WITH\n",
    "(external_location='s3://bucket/path/',\n",
    "format='PARQUET',\n",
    "write_compression='SNAPPY',\n",
    "partitioned_by = ARRAY['yr_mo_partition'])\n",
    "AS\n",
    "SELECT DISTINCT\n",
    "    latitude,\n",
    "    longitude,\n",
    "    temp AS temp_F,\n",
    "    (temp - 32) * (5.0/9.0) AS temp_C,\n",
    "    row_ts,\n",
    "    time,\n",
    "    SUBSTRING(time,1,7) AS yr_mo_partition\n",
    "FROM source_table\n",
    "\"\"\"\n",
    "# ...\n",
    "```\n",
    "\n",
    "#### c. DQ Checks Job\n",
    "Validates data quality.\n",
    "\n",
    "```python\n",
    "# dq_checks_parquet_weather_table.py\n",
    "import sys\n",
    "import awswrangler as wr\n",
    "\n",
    "NULL_DQ_CHECK = \"\"\"\n",
    "SELECT \n",
    "    SUM(CASE WHEN temp_C IS NULL THEN 1 ELSE 0 END) AS res_col\n",
    "FROM database_name.table_name\n",
    "\"\"\"\n",
    "# ...\n",
    "```\n",
    "\n",
    "#### d. Publish Job\n",
    "Creates the production-ready dataset.\n",
    "\n",
    "```python\n",
    "# publish_prod_parquet_weather_table.py\n",
    "import sys\n",
    "import boto3\n",
    "\n",
    "# Create production table\n",
    "client = boto3.client('athena')\n",
    "queryString = \"\"\"\n",
    "CREATE TABLE prod_table WITH\n",
    "(external_location='s3://prod-bucket/path/',\n",
    " format='PARQUET',\n",
    " write_compression='SNAPPY',\n",
    " partitioned_by = ARRAY['yr_mo_partition'])\n",
    "AS\n",
    "SELECT * FROM transformed_table\n",
    "\"\"\"\n",
    "# ...\n",
    "```\n",
    "\n",
    "### 6. AWS Glue Workflow\n",
    "\n",
    "The entire pipeline is orchestrated using an AWS Glue Workflow, which runs the jobs in sequence.\n",
    "\n",
    "![Glue Workflow Screenshot 1](pics/workflow-screenshot1.png)\n",
    "\n",
    "![Glue Workflow Screenshot 2](pics/workflow-screenshot2.png)\n",
    "\n",
    "### 7. Athena Tables\n",
    "\n",
    "The processed data can be queried using Amazon Athena:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM \"weather-database-04142025\".\"open_meteo_weather_data_parquet_tbl\" LIMIT 10;\n",
    "```\n",
    "\n",
    "![Athena Query Results Screenshot](pics/athena-results.png)\n",
    "\n",
    "### 8. Grafana Dashboard\n",
    "\n",
    "The final data is visualized in a Grafana dashboard showing temperature trends over time.\n",
    "\n",
    "![Grafana Dashboard Screenshot](pics/grafana-screenshot.png)\n",
    "\n",
    "## Issues and Resolutions\n",
    "\n",
    "### Issue 1: Lambda IAM Permissions\n",
    "\n",
    "**Problem**: Lambda function failed with `AccessDeniedException` when calling the `PutRecordBatch` operation for Firehose.\n",
    "\n",
    "**Resolution**: Added the required `firehose:PutRecordBatch` permission to the Lambda execution role.\n",
    "\n",
    "### Issue 2: Duplicate Records\n",
    "\n",
    "**Problem**: When rerunning the pipeline, data records were duplicated (212 records became 424).\n",
    "\n",
    "**Resolution**: \n",
    "- Added `DISTINCT` to the SELECT statement in the create job\n",
    "- Implemented proper cleanup of previous data before creating new tables\n",
    "- Used a two-step process: first drop the table, then create a new one\n",
    "\n",
    "### Issue 3: Glue Crawler Cancellation\n",
    "\n",
    "**Problem**: Glue Crawler was showing \"CANCELLING\" status.\n",
    "\n",
    "**Resolution**: Ensured data was present in the S3 bucket before running the crawler.\n",
    "\n",
    "### Issue 4: SQL Syntax Errors\n",
    "\n",
    "**Problem**: Error with database names containing hyphens.\n",
    "\n",
    "**Resolution**: Modified SQL queries to handle database names with hyphens correctly.\n",
    "\n",
    "### Issue 5: Data Quality Check Database Reference\n",
    "\n",
    "**Problem**: DQ checks job was referencing a non-existent database.\n",
    "\n",
    "**Resolution**: Updated the script to use the correct database name for consistency across all components.\n",
    "\n",
    "## Setup and Configuration\n",
    "\n",
    "1. Configure AWS CLI with appropriate credentials\n",
    "2. Create S3 buckets for source data, transformed data, and query results\n",
    "3. Set up Lambda function with required permissions\n",
    "4. Configure Kinesis Firehose delivery stream\n",
    "5. Create Glue Crawler to catalog the data\n",
    "6. Deploy Glue jobs for data transformation\n",
    "7. Set up Glue Workflow to orchestrate the pipeline\n",
    "8. Configure Grafana for data visualization\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- AWS Lambda\n",
    "- Amazon Kinesis Firehose\n",
    "- Amazon S3\n",
    "- AWS Glue\n",
    "- Amazon Athena\n",
    "- Grafana\n",
    "- Python libraries:\n",
    "  - boto3\n",
    "  - urllib3\n",
    "  - awswrangler\n",
    "\n",
    "## Further Improvements\n",
    "\n",
    "- Add alerting for data quality issues\n",
    "- Implement incremental data loading\n",
    "- Add weather forecast data\n",
    "- Create additional visualizations for deeper insights\n",
    "- Automate deployment using Infrastructure as Code (CloudFormation/CDK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f718c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
